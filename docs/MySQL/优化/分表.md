# 分表

当业务规模达到一定规模之后，像日订单量达到千万级别，必须进行分库分表，但分库分表之后会产生一些问题，比如，
- 唯一主键
- 分页查询


### 唯一主键
单表可以用自增主键，但分表后就不行了，最简单的办法就是用一个唯一的业务字段作为唯一主键，比如：订单表的订单号。这种唯一 ID 也叫做分布式 ID、全局 ID。

生成分布式 ID 的方式有很多种，参考：[分布式 ID](https://github.com/liangkuai/architecture)


### 分表
分表怎么分？

#### 1. 首先根据自身的业务量和增量考虑
比如，如果现在的日订单量是 10 万，预计一年后达到日 100 万。**根据业务属性，一般我们就支持查询半年内的订单，超过半年的订单需要做归档处理**。那么以日订单量 100 万的数量级来看，半年的订单量就有 100万 * 180 = 1.8亿。单表肯定扛不住，更别说 RT 时间了。

根据经验，单表几百万对数据库压力不大，那么只要分 256 张表就足够了，1.8亿 / 256 = 70万。为了保险起见，也可以分到 512 张表。如果业务量增长到每天 1000万，分 1024 张表更合适。

通过分表加上超过半年的数据归档之后，单表 70 万的数据就足以应对大部分场景了。接下来对订单号 hash，然后对 256 取模的就可以落到具体的哪张表了。一般我们把分表的字段叫做 sharding key，也就是这里的订单号。


### 分表后的查询

#### 带 sharding key 的查询
不管是什么查询都能定位到具体的表

#### 不带 sharding key 的查询
如果用订单号作为 sharding key，但是 APP、小程序这种一般都是通过用户 ID 查询。除了有的公司或者业务会用用户 ID 作 sharding key，这种没问题；还有一种简单的办法就是，在订单上带上用户 ID 的属性。

比如，使用 Snowflake 算法生成的订单号中，时间戳有 41 位，可以取 10 位存储用户 ID。在数据落表的时候，对订单号中的用户 ID hash 取模，这样就跟 shardinng key 查询一样了。

#### 复杂查询
无论是订单号还是用户 ID 作为 sharding key，按照以上的两种方式都可以解决问题。但是如果既不是订单号又不是用户 ID 查询怎么办？最直观的例子就是来自商户端或者管理后台的查询，商户端都是以商户或卖家的 ID 作为查询条件来查的，管理后台的查询条件可能就更复杂了，可能会有十几个条件。

#### B 端查询
##### 1. 双写
订单数据写两份，C 端和 B 端各一份，C 端可以用订单号、用户 ID 做 sharding key，B 端就用商户 ID 作为 sharding key。

至于说双写可能会影响性能，因为对于 B 端来说轻微的延迟是可以接受的，所以可以采取异步的方式去落 B 端订单。就像在淘宝买个东西下单了，卖家稍微延迟个一两秒收到这个订单，没什么太大影响。

##### 2. 离线数仓 / ES
订单数据落库之后，通过 binlog 或者 MQ 消息的形式，把数据同步到数仓或者 ES，用这两种方式对海量数据进行存储和查询很简单。不过这种方式肯定会有一定延迟，但是这种可控范围的延迟是可以接受的。

#### 管理后台
针对管理后台的查询，比如运营、业务、产品需要看数据，查询条件可能比较复杂，用 ES 或者数仓都可以做到。

如果不用上述方案，又要不带 sharding key 的分页查询，那就只能扫全表查询聚合数据，然后手动做分页了，但是这样查出来的结果是有限制的。比如 256 个分片，查询的时候循环扫描所有的分片，每个片取 20 条数据，最后聚合数据手工分页，那必然是不可能查到全量的数据的。

### 总结
分库分表后的查询问题，对于有经验的同学来说都知道，但其实大部分同学做的业务可能都没来到这个数量级，分库分表可能都停留在概念阶段，面试被问到后就手足无措了，因为没有经验不知道怎么办。

分库分表首先是基于现有的业务量和未来的增量做出判断，比如：pdd 这种日订单量 5000w+，半年数据得有百亿级别了，那就得分到 4096 张表，但是实际的操作是一样的，需要根据业务做出合理的选择。

对于基于 sharding key 的查询我们可以很简单的解决，对于非 sharding key 的查询可以通过落双份数据和数仓、ES的方案来解决，当然，如果分表后数据量很小的话，建好索引，扫全表查询其实也不是什么问题。



### 参考
- [百亿级数据分表后怎么分页查询？ - 艾小仙的文章 - 知乎](https://zhuanlan.zhihu.com/p/280674615)